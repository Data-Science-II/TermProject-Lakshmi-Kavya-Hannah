# -*- coding: utf-8 -*-
"""data science 2 project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nmg8xgh3CNJzMNDIoAdwTrgNksAVEy-O
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
import sklearn.metrics as metrics
from sklearn.metrics import classification_report
# test classification dataset
from sklearn.datasets import make_classification
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier
import sklearn.metrics as metrics
from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier


url1 = 'https://raw.githubusercontent.com/lsharples1/Data_Science_Competition/main/Simulated_Data_Train.csv?token=AO4LMYK744PF4GAEXCQHBZDAPSKSM'
train = pd.read_csv(url1)

url2 = 'https://raw.githubusercontent.com/lsharples1/Data_Science_Competition/main/Simulated_Data_Test.csv'
test = pd.read_csv(url2)

url3 = 'https://raw.githubusercontent.com/lsharples1/Data_Science_Competition/main/Simulated_Data_Validation.csv?token=AO4LMYMGZVCQVZJ6P6TBBS3APSKXS'
validate = pd.read_csv(url3)
y_Train = train['Default_ind']
x_Train = train
x_Train.drop('Default_ind', axis='columns',inplace=True)
x_Train['uti_card_50plus_pct'].fillna((x_Train['uti_card_50plus_pct'].mean()), inplace=True) 
x_Train['rep_income'].fillna((x_Train['rep_income'].mean()), inplace=True) 
x_Train = pd.get_dummies(x_Train, prefix_sep='_', drop_first=False)
scaler = preprocessing.StandardScaler().fit(x_Train)
x_TrainScale = scaler.transform(x_Train)
smote = SMOTE()
#x_TrainOV, y_TrainOV = smote.fit_resample(x_Train, y_Train)
y_Test = test['Default_ind']
x_Test = test
x_Test.drop('Default_ind', axis='columns',inplace=True)
x_Test = pd.get_dummies(x_Test, prefix_sep='_', drop_first=False)
x_Test['uti_card_50plus_pct'].fillna((x_Test['uti_card_50plus_pct'].mean()), inplace=True) 
x_Test['rep_income'].fillna((x_Test['rep_income'].mean()), inplace=True) 
scaler2 = preprocessing.StandardScaler().fit(x_Test)
x_TestScale = scaler2.transform(x_Test)
y_Val = validate['Default_ind']
x_Val = validate
x_Val.drop('Default_ind', axis='columns',inplace=True)
x_Val = pd.get_dummies(x_Val, prefix_sep='_', drop_first=False)
x_Val['uti_card_50plus_pct'].fillna((x_Val['uti_card_50plus_pct'].mean()), inplace=True) 
x_Val['rep_income'].fillna((x_Val['rep_income'].mean()), inplace=True) 
scaler3 = preprocessing.StandardScaler().fit(x_Val)
x_ValScale = scaler3.transform(x_Val)



train.head()

#EXPLOTORY DATA ANALYSIS 
def quantitative_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', ax=None, verbose=True, swarm=False):
   
    series = dataframe[y]
    print(series.describe())
    print('mode: ', series.mode())
    if verbose:
        print('='*80)
        print(series.value_counts())

    sns.boxplot(x=x, y=y, hue=hue, data=dataframe, palette=palette, ax=ax)

    if swarm:
        sns.swarmplot(x=x, y=y, hue=hue, data=dataframe,
                      palette=palette, ax=ax)

    plt.show()

c_palette = ['tab:blue', 'tab:orange']
quantitative_summarized(dataframe= train, y = 'credit_age', x = 'Default_ind', palette=c_palette, verbose=False, swarm=True)

#MODEL 1- LOGISTIC REGRESSION

logReg = LogisticRegression()
logReg.fit(x_TrainScale, y_Train)
predictedVAL = logReg.predict(x_ValScale)
predictedTEST = logReg.predict(x_TestScale)
accuracyLRVAL = metrics.accuracy_score(y_Val, predictedVAL)
accuracyLRTEST = metrics.accuracy_score(y_Test, predictedTEST)
print(classification_report(y_Test, logReg.predict(x_TestScale)))

#MODEL 2- RANDOM FOREST
# Import the model we are using

# Instantiate model with 1000 decision trees
rf = RandomForestClassifier(n_estimators = 64)
# Train the model on training data
rf = RandomForestClassifier(n_estimators = 64)
rf.fit(x_Train,y_Train)
predictedRFVAL = rf.predict(x_Val)
predictedRFTEST = rf.predict(x_Test)
accuracyRFVAL = metrics.accuracy_score(y_Val, predictedRFVAL)
accuracyRFTEST = metrics.accuracy_score(y_Test, predictedRFTEST)
print(classification_report(y_Val, rf.predict(x_Val)))

#FEATURE IMPORTANCE FOR RANDOM FOREST
# Extract feature importances
importanceRF = pd.DataFrame({'feature': list(x_Train.columns),
                   'importance': rf.feature_importances_}).\
                    sort_values('importance', ascending = False)

# Display
print(importanceRF)

#FEATURE IMPORTANCE FOR LOGISTIC REGRESSION
importanceLR = pd.DataFrame({'feature': list(x_Train.columns),
                   'importance': logReg.coef_[0]}).\
                    sort_values('importance', ascending = False)

# Display
print(importanceLR)

#GRADIET BOOSTING MODEL

lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]

for learning_rate in lr_list:
    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)
    gb_clf.fit(x_Train, y_Train)

    print("Learning rate: ", learning_rate)
    print("Accuracy score (training): {0:.3f}".format(gb_clf.score(x_Train, y_Train)))
    print("Accuracy score (validation): {0:.3f}".format(gb_clf.score(x_Val, y_Val)))

gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)
gb_clf2.fit(x_Train, y_Train)
predictions = gb_clf2.predict(x_Val)

predictedGBVAL = gb_clf2.predict(x_Val)
predictedGBTEST = gb_clf2.predict(x_Test)
accuracyGBVAL = metrics.accuracy_score(y_Val, predictedGBVAL)
accuracyGBTEST = metrics.accuracy_score(y_Test, predictedGBTEST)



print("Confusion Matrix:")
print(confusion_matrix(y_Val, predictions))

print("Classification Report")
print(classification_report(y_Val, predictions))

importanceGB = pd.DataFrame({'feature': list(x_Train.columns),
                   'importance': gb_clf2.feature_importances_}).\
                    sort_values('importance', ascending = False)

# Display
print(importanceGB)

#CONFUSION MATRICES

def plot(classifier, x, y, title): 
    class_names = ["Not Defaulted", "Defaulted"]
    disp = plot_confusion_matrix(classifier, x, y,
                                 display_labels=class_names,
                                 cmap=plt.cm.PuRd, values_format = ''
                                   )
    disp.ax_.set_title(title)
    plt.show() 

plot(logReg, x_Test, y_Test, "Logistic Regression Model-scaled")
print("Test set accuracy:") #before smote- .9311, after smote- .87
print(accuracyLRTEST)
print("Validation set accuracy:") #before smote- .934, after smote- .8, 
print(accuracyLRVAL)
plot(rf, x_Test, y_Test, "Random Forest Model-original")
print("Test set accuracy:") #before smote- 1.0, after smote- 1.0
print(accuracyRFTEST)
print("Validation set accuracy:") #before smote- .942, after smote- .936
print(accuracyRFVAL)
plot(gb_clf,x_Test, y_Test, "Gradient Boosting")
print("Test set accuracy:") 
print(accuracyGBTEST)
print("Validation set accuracy:") 
print(accuracyGBVAL)